---
{"dg-publish":true,"dg-path":"è®¡ç®—æœº/æœºå™¨å­¦ä¹ /çº¿æ€§å›å½’.md","permalink":"/è®¡ç®—æœº/æœºå™¨å­¦ä¹ /çº¿æ€§å›å½’/","dgPassFrontmatter":true,"noteIcon":"","created":"2024-08-29T21:21:47.821+08:00","updated":"2025-04-17T18:41:04.610+08:00"}
---


(terminology::**Linear Regression**) 
æ˜¯ä¸€ç§é€šè¿‡å±æ€§çš„çº¿æ€§ç»„åˆæ¥è¿›è¡Œé¢„æµ‹çš„çº¿æ€§æ¨¡å‹ã€‚å…¶ç›®çš„æ˜¯æ‰¾åˆ°ä¸€æ¡ç›´çº¿æˆ–è€…ä¸€ä¸ªå¹³é¢æˆ–è€…æ›´é«˜ç»´çš„è¶…å¹³é¢ï¼Œ**ä½¿å¾—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°åŒ–**ã€‚

$$\begin{align}
\hat{y}=h(x)=w_{0}x_{0}+w_{1}x_{1}+\cdots + w_{n}x_{n}=w^{T}x
\end{align}$$
$$\begin{align}
J(w)= \dfrac{1}{2} \sum\limits_{i=1}^{m} (h(x)_{i}-y_{i})^{2}
\end{align}$$
$m$ ä»£è¡¨è®­ç»ƒé›†ä¸­æ ·æœ¬çš„æ•°é‡     $n$ ä»£è¡¨ç‰¹å¾çš„æ•°é‡ 
$x$ ä»£è¡¨ç‰¹å¾/è¾“å…¥å˜é‡      $y$ ä»£è¡¨ç›®æ ‡å˜é‡/è¾“å‡ºå˜é‡
$(x, y)$ ä»£è¡¨è®­ç»ƒé›†ä¸­çš„æ ·æœ¬    $(x_{i},y_{i})$ ä»£è¡¨ç¬¬ $i$ ä¸ªè§‚å¯Ÿæ ·æœ¬ 
  $\hat{y}=h(x)$ ä»£è¡¨é¢„æµ‹çš„å€¼   $h$ ä»£è¡¨å­¦ä¹ ç®—æ³•çš„è§£å†³æ–¹æ¡ˆæˆ–å‡½æ•°ä¹Ÿç§°ä¸ºå‡è®¾ï¼ˆhypothesisï¼‰    
$J(w)$ ä¸ºæŸå¤±å‡½æ•°ï¼Œé‡‡ç”¨å¹³æ–¹å’ŒæŸå¤±ï¼Œæ®‹å·®å¹³æ–¹å’Œ
### ä¸€ã€æœ€å°äºŒä¹˜æ³• LSM
ä¸éœ€è¦é€‰æ‹©å­¦ä¹ ç‡,éœ€è¦è®¡ç®— $(X^{T}X)^{-1}$,åªé€‚ç”¨äºçº¿æ€§æ¨¡å‹ ï¼Œä¸é€‚åˆé€»è¾‘å›å½’æ¨¡å‹ç­‰å…¶ä»–æ¨¡å‹

ç®—æ³•æµç¨‹ï¼šçŸ¥ $h(x)$ï¼Œå¯»æ‰¾ä¸€ç»„ $w(w_{0},w_{1},\cdots, w_{n})$ ä½¿å¾—æ®‹å·®å¹³æ–¹å’Œ $J(w)$ æœ€å°
$$\begin{align}
\sum z_{i}^{2}=z^{T}z\quad \dfrac{\partial X^{T}X}{\partial X}=2X \quad  \dfrac{\partial AX}{\partial X}=A^{T}\quad \dfrac{\partial X^{T}AX}{\partial X}=(A+A^{T})X  
\end{align}$$

$$\begin{align}
\dfrac{\partial J(w)}{\partial w}= \dfrac{1}{2} \dfrac{\partial  }{\partial w}(Xw-Y)^{T}(Xw-Y)=\dfrac{1}{2} \dfrac{\partial }{\partial w}(w^{T}X^{T}Xw-2w^{T}X^{T}Y+Y^{T}Y)=\dfrac{1}{2}(2X^{T}Xw-2X^{T}Y+0)=0   
\end{align}$$

$X^{T}Xw-X^{T}Y=0\quad \Rightarrow \quad w=(X^{T}X)^{-1}X^{T}Y$
### äºŒã€æ¢¯åº¦ä¸‹é™ 
éœ€è¦é€‰æ‹©å­¦ä¹ ç‡ $\alpha$ï¼Œéœ€è¦å¤šæ¬¡è¿­ä»£ï¼Œå½“ç‰¹å¾æ•°é‡ğ‘›å¤§æ—¶ä¹Ÿèƒ½è¾ƒå¥½é€‚ç”¨ï¼Œé€‚ç”¨äºå„ç§ç±»å‹çš„æ¨¡å‹

- æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆBatch Gradient Descent,BGDï¼‰ï¼šæ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œéƒ½ç”¨åˆ°äº†**æ‰€æœ‰**çš„è®­ç»ƒæ ·æœ¬
- éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descent,SGDï¼‰ï¼šæ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œç”¨åˆ°**ä¸€ä¸ª**æ ·æœ¬ï¼Œåœ¨æ¯ä¸€æ¬¡è®¡ç®—ä¹‹åä¾¿æ›´æ–°å‚æ•°ï¼Œè€Œä¸éœ€è¦é¦–å…ˆå°†æ‰€æœ‰çš„è®­ç»ƒé›†æ±‚å’Œ
- å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ï¼ˆMini-Batch Gradient Descent,MBGDï¼‰ï¼šæ¢¯åº¦ä¸‹é™çš„æ¯ä¸€æ­¥ä¸­ï¼Œç”¨åˆ°äº†**ä¸€å®šæ‰¹é‡**çš„è®­ç»ƒæ ·æœ¬
### ä¸‰ã€å›å½’çš„è¯„ä»·æŒ‡æ ‡

å‡æ–¹è¯¯å·® MSEï¼ˆMean Square Errorï¼‰ï¼š$\dfrac{1}{m} \sum\limits_{i=1}^{m}(y_{i}-\hat{y}_{i})^{2}$

å‡æ–¹æ ¹è¯¯å·® RMSE (Root Mean Square Error): $\sqrt{ \dfrac{1}{m} \sum\limits_{i=1}^{m} (y_{i}-\hat{y}_{i})^{2} }$

å¹³å‡ç»å¯¹è¯¯å·®MAEï¼ˆMean Absolute Error): $\dfrac{1}{m} \sum\limits_{i=1}^{m}\left\lvert  y_{i}-\hat{y}_{i} \right\rvert$

RSquare $R^{2}= \dfrac{SSR}{SST}=1- \dfrac{SSE}{S S T}=1- \dfrac{MSE}{Var}$, è¶Šæ¥è¿‘äº 1, è¯´æ˜æ¨¡å‹æ‹Ÿåˆå¾—è¶Šå¥½

$$\begin{align}
SS R= \sum\limits_{i=1}^{m}(\hat{y}_{i}-\bar{y})^{2}\quad  S S E=\sum\limits_{i=1}^{m}(\hat{y}_{i}-y)^{2}\quad  S S T=\sum\limits_{i=1}^{m}({y}_{i}-\bar{y})^{2}
\end{align}$$
